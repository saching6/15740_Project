{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code implements the training pipeline for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from dataformatter import *\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all the hyper-parameters that need to be set here\n",
    "BATCH_SZ = 512\n",
    "MODEL_TYPE = 'TRANSFORMER' # OPTIONS ARE : [FC, TRANSFORMER, LSTM]\n",
    "LR = 5e-4\n",
    "DATA_PATH = 'hawkeye_trace_belady_graph.csv' # This is the CSV FILE WE ARE TRYING TO ANALYZE\n",
    "SAVE_FLDR = 'results'\n",
    "N_EPOCHS = 30\n",
    "MODEL_DESC='{} MODEL'.format(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, data_iterator, mode='train'):\n",
    "    model.train()\n",
    "    if mode == 'val':\n",
    "        model.eval()\n",
    "    stats = []\n",
    "    num_egs = 0\n",
    "    for batch in data_iterator:\n",
    "        # we get the loss from passing the batch to the model\n",
    "        # each model will have it's own way of deadling with the data [we can jointly figure this out]\n",
    "        loss, acc, bsz = model(batch)\n",
    "        stats.append([loss.item(), acc.item()])\n",
    "        num_egs += bsz\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    stats = np.array(stats)\n",
    "    avg_loss = np.mean(stats[:, 0])\n",
    "    avg_acc = (stats[:, 1]).sum() / num_egs\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, dataset, num_epochs=20, desc='Description of model', shuffle=True):\n",
    "    # Todo [all]\n",
    "    # Figure out how to split the data into a train-val-test regime\n",
    "    stats = []\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "        epoch_stats = run_epoch(model, optimizer, data_iter, mode='train')\n",
    "        stats.append(epoch_stats)\n",
    "        print('Epoch {} : Avg Loss = {} Avg Acc = {}'.format(epoch_, stats[-1][0], stats[-1][1]))\n",
    "    stats = np.array(stats)\n",
    "    graph_results(stats, desc)\n",
    "    return model\n",
    "\n",
    "def set_wise_trainer(model, optimizer, setwise_dataset, num_epochs=20, desc='Set-Wise Model', shuffle=False):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        accs = []\n",
    "        for set_id, dataset in setwise_dataset.items():\n",
    "            model.remap_embedders(dataset, set_id)\n",
    "            data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "            this_stats = run_epoch(model, optimizer, data_iter, mode='train')\n",
    "            stats[set_id].append(this_stats)\n",
    "            accs.append(this_stats[-1])\n",
    "        acc_stats = np.min(accs), np.mean(accs), np.median(accs), np.max(accs)\n",
    "        print('Min Acc {} | Mean Acc : {} | Median Acc {} | Max Acc {} '.format(*acc_stats))\n",
    "    return model, stats\n",
    "\n",
    "def evaluate(model, dataset, epoch_id=-1, print_res=True, shuffle=False):\n",
    "    # get a data iterator for this epoch\n",
    "    model.eval()\n",
    "    data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "    epoch_stats = run_epoch(model, optimizer, data_iter, mode='test')\n",
    "    if print_res:\n",
    "        print('Epoch {} : Avg Loss = {} Avg Acc = {}'.format(epoch_, epoch_stats[0], epoch_stats[1]))\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Program Counter': 0, 'Set': 1, 'Cache Friendly': 2}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAVE_FLDR):\n",
    "    os.makedirs(SAVE_FLDR)\n",
    "\n",
    "model = get_model(MODEL_TYPE)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "chosen_columns = model.get_data_columns()\n",
    "dataset = csv_to_data(DATA_PATH, chosen_columns)\n",
    "print(model.feat_idx_map)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the average accuracy :  0.7159665140267546  From predicting all zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([14336])) that is different to the input size (torch.Size([14336, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Avg Loss = 0.614604903945645 Avg Acc = 0.7108714702886616\n",
      "Epoch 1 : Avg Loss = 0.5989321776964132 Avg Acc = 0.7156103718793343\n",
      "Epoch 2 : Avg Loss = 0.5983402642231543 Avg Acc = 0.7155663520717753\n",
      "Epoch 3 : Avg Loss = 0.5969935913109085 Avg Acc = 0.7170176512656034\n",
      "Epoch 4 : Avg Loss = 0.5977325167470765 Avg Acc = 0.7157691818004508\n",
      "Epoch 5 : Avg Loss = 0.5972964757854499 Avg Acc = 0.716375300689147\n",
      "Epoch 6 : Avg Loss = 0.597133259460764 Avg Acc = 0.7162151363124133\n",
      "Epoch 7 : Avg Loss = 0.5969956187368597 Avg Acc = 0.7165943838852289\n",
      "Epoch 8 : Avg Loss = 0.5972772084393547 Avg Acc = 0.7161003461988558\n"
     ]
    }
   ],
   "source": [
    "model.prep_for_data(dataset)\n",
    "average_pred = np.mean(dataset[:, -1])\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.use_cuda = torch.cuda.is_available()\n",
    "print('This is the average accuracy : ', 1.0 - average_pred, ' From predicting all zeros')\n",
    "model = trainer(model, optimizer, dataset, num_epochs=N_EPOCHS, desc=MODEL_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max key 1807.0. Min 25, Mean 51.50244140625, Median 50.0, Max 115\n",
      "1843 205\n",
      "Logging Pre-Training Performance\n",
      "40 [0.87297585 0.45507812 0.70132639]\n",
      "81 [0.86061046 0.45904064 0.68750592]\n",
      "122 [0.85533392 0.46460895 0.68938448]\n",
      "163 [0.8644205  0.4499297  0.70283787]\n",
      "204 [0.87130949 0.44881025 0.70726576]\n",
      "Average Stats Before Training :  [0.87155967 0.4488313  0.70706772]\n",
      "Min Acc 0.25 | Mean Acc : 0.624512368882914 | Median Acc 0.625 | Max Acc 0.96875 \n",
      "Min Acc 0.28125 | Mean Acc : 0.6436975470581894 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6441128122977651 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6455715436343803 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6436549225179562 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6449629183340161 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6446413525550112 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6443906613941192 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.21875 | Mean Acc : 0.6454899647092859 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.21875 | Mean Acc : 0.6443356494620321 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6434970836887467 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6435442416977848 | Median Acc 0.65625 | Max Acc 0.96875 \n"
     ]
    }
   ],
   "source": [
    "setwise_dataset = group_by_set(dataset)\n",
    "keys = list(setwise_dataset.keys())\n",
    "num_tr = int(0.9 * len(keys))\n",
    "train_keys, val_keys = keys[:num_tr], keys[num_tr:]\n",
    "vals = list([len(x) for k, x in setwise_dataset.items()])\n",
    "max_key = keys[np.argmax(vals)]\n",
    "print('Max key {}. Min {}, Mean {}, Median {}, Max {}'.format(max_key, np.min(vals), np.mean(vals), np.median(vals), np.max(vals)))\n",
    "model.prep_for_data(setwise_dataset[max_key])\n",
    "print(len(train_keys), len(val_keys))\n",
    "# Logging-pre-training performance\n",
    "print('Logging Pre-Training Performance')\n",
    "all_stats = []\n",
    "for id_, set_id in enumerate(val_keys):\n",
    "    if (id_ + 1) % int(len(val_keys) // 5) == 0:\n",
    "        print(id_, np.array(all_stats).mean(axis=0))\n",
    "    this_dataset = setwise_dataset[set_id]\n",
    "    model.remap_embedders(this_dataset, set_id)\n",
    "    result = evaluate(model, this_dataset, print_res=False)\n",
    "    average_pred = np.mean(this_dataset[:, -1])\n",
    "    all_stats.append([*result, 1.0 - average_pred])\n",
    "av_res = np.mean(all_stats, axis=0)\n",
    "print('Average Stats Before Training : ', av_res)\n",
    "train_setwise = {k: setwise_dataset[k] for k in train_keys}\n",
    "model, stats = set_wise_trainer(model, optimizer, train_setwise, num_epochs=N_EPOCHS)\n",
    "\n",
    "print('Logging Post-Training Performance')\n",
    "all_stats = []\n",
    "for id_, set_id in enumerate(val_keys):\n",
    "    if (id_ + 1) % int(len(val_keys) // 5) == 0:\n",
    "        print(id_, np.array(all_stats).mean(axis=0))\n",
    "    this_dataset = setwise_dataset[set_id]\n",
    "    model.remap_embedders(this_dataset, set_id)\n",
    "    result = evaluate(model, this_dataset, print_res=False)\n",
    "    average_pred = np.mean(this_dataset[:, -1])\n",
    "    all_stats.append([*result, 1.0 - average_pred])\n",
    "av_res = np.mean(all_stats, axis=0)\n",
    "print('Average Stats After Training : ', av_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
