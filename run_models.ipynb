{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code implements the training pipeline for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from dataformatter import *\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all the hyper-parameters that need to be set here\n",
    "BATCH_SZ = 32\n",
    "MODEL_TYPE = 'FC' # OPTIONS ARE : [FC, TRANSFORMER, LSTM]\n",
    "LR = 5e-4\n",
    "DATA_PATH = 'hawkeye_trace_belady_graph.csv' # This is the CSV FILE WE ARE TRYING TO ANALYZE\n",
    "SAVE_FLDR = 'results'\n",
    "N_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, data_iterator, mode='train'):\n",
    "    model.train()\n",
    "    if mode == 'val':\n",
    "        model.eval()\n",
    "    stats = []\n",
    "    num_egs = 0\n",
    "    for batch in data_iterator:\n",
    "        # we get the loss from passing the batch to the model\n",
    "        # each model will have it's own way of deadling with the data [we can jointly figure this out]\n",
    "        loss, acc = model(batch)\n",
    "        stats.append([loss.item(), acc.item()])\n",
    "        num_egs += len(batch)\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    stats = np.array(stats)\n",
    "    avg_loss = np.mean(stats[:, 0])\n",
    "    avg_acc = (stats[:, 1]).sum() / num_egs\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, dataset, num_epochs=20, desc='Description of model', shuffle=True):\n",
    "    # Todo [all]\n",
    "    # Figure out how to split the data into a train-val-test regime\n",
    "    stats = []\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "        epoch_stats = run_epoch(model, optimizer, data_iter, mode='train')\n",
    "        stats.append(epoch_stats)\n",
    "        print('Epoch {} : Avg Loss = {} Avg Acc = {}'.format(epoch_, stats[-1][0], stats[-1][1]))\n",
    "    stats = np.array(stats)\n",
    "    graph_results(stats, desc)\n",
    "    return model\n",
    "\n",
    "def set_wise_trainer(model, optimizer, setwise_dataset, num_epochs=20, desc='Set-Wise Model', shuffle=False):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        accs = []\n",
    "        for set_id, dataset in setwise_dataset.items():\n",
    "            model.remap_embedders(dataset, set_id)\n",
    "            data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "            this_stats = run_epoch(model, optimizer, data_iter, mode='train')\n",
    "            stats[set_id].append(this_stats)\n",
    "            accs.append(this_stats[-1])\n",
    "        acc_stats = np.min(accs), np.mean(accs), np.median(accs), np.max(accs)\n",
    "        print('Min Acc {} | Mean Acc : {} | Median Acc {} | Max Acc {} '.format(*acc_stats))\n",
    "    return model, stats\n",
    "\n",
    "def evaluate(model, dataset, epoch_id=-1, print_res=True, shuffle=False):\n",
    "    # get a data iterator for this epoch\n",
    "    model.eval()\n",
    "    data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "    epoch_stats = run_epoch(model, optimizer, data_iter, mode='test')\n",
    "    if print_res:\n",
    "        print('Epoch {} : Avg Loss = {} Avg Acc = {}'.format(epoch_, epoch_stats[0], epoch_stats[1]))\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-021beb3e3e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mchosen_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_idx_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldery/computer_architecture/15740_Project/dataformatter.py\u001b[0m in \u001b[0;36mcsv_to_data\u001b[0;34m(fname, chosen_columns)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mheader_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheader_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchosen_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_npy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldery/computer_architecture/15740_Project/dataformatter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mheader_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheader_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchosen_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_npy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: '' is not in list"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAVE_FLDR):\n",
    "    os.makedirs(SAVE_FLDR)\n",
    "\n",
    "model = get_model(MODEL_TYPE)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "chosen_columns = model.get_data_columns()\n",
    "dataset = csv_to_data(DATA_PATH, chosen_columns)\n",
    "model.rm_feature('Set')\n",
    "print(model.feat_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.prep_for_data(dataset)\n",
    "# average_pred = np.mean(dataset[:, -1])\n",
    "# print('This is the average accuracy : ', 1.0 - average_pred, ' From predicting all zeros')\n",
    "# model = trainer(model, optimizer, dataset, num_epochs=N_EPOCHS, desc='Basic MLP Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max key 1807.0. Min 25, Mean 51.50244140625, Median 50.0, Max 115\n",
      "1843 205\n",
      "Logging Pre-Training Performance\n",
      "40 [0.87297585 0.45507812 0.70132639]\n",
      "81 [0.86061046 0.45904064 0.68750592]\n",
      "122 [0.85533392 0.46460895 0.68938448]\n",
      "163 [0.8644205  0.4499297  0.70283787]\n",
      "204 [0.87130949 0.44881025 0.70726576]\n",
      "Average Stats Before Training :  [0.87155967 0.4488313  0.70706772]\n",
      "Min Acc 0.25 | Mean Acc : 0.624512368882914 | Median Acc 0.625 | Max Acc 0.96875 \n",
      "Min Acc 0.28125 | Mean Acc : 0.6436975470581894 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6441128122977651 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6455715436343803 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6436549225179562 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6449629183340161 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6446413525550112 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6443906613941192 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.21875 | Mean Acc : 0.6454899647092859 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.21875 | Mean Acc : 0.6443356494620321 | Median Acc 0.65625 | Max Acc 0.96875 \n",
      "Min Acc 0.25 | Mean Acc : 0.6434970836887467 | Median Acc 0.65625 | Max Acc 1.0 \n",
      "Min Acc 0.25 | Mean Acc : 0.6435442416977848 | Median Acc 0.65625 | Max Acc 0.96875 \n"
     ]
    }
   ],
   "source": [
    "setwise_dataset = group_by_set(dataset)\n",
    "keys = list(setwise_dataset.keys())\n",
    "num_tr = int(0.9 * len(keys))\n",
    "train_keys, val_keys = keys[:num_tr], keys[num_tr:]\n",
    "vals = list([len(x) for k, x in setwise_dataset.items()])\n",
    "max_key = keys[np.argmax(vals)]\n",
    "print('Max key {}. Min {}, Mean {}, Median {}, Max {}'.format(max_key, np.min(vals), np.mean(vals), np.median(vals), np.max(vals)))\n",
    "model.prep_for_data(setwise_dataset[max_key])\n",
    "print(len(train_keys), len(val_keys))\n",
    "# Logging-pre-training performance\n",
    "print('Logging Pre-Training Performance')\n",
    "all_stats = []\n",
    "for id_, set_id in enumerate(val_keys):\n",
    "    if (id_ + 1) % int(len(val_keys) // 5) == 0:\n",
    "        print(id_, np.array(all_stats).mean(axis=0))\n",
    "    this_dataset = setwise_dataset[set_id]\n",
    "    model.remap_embedders(this_dataset, set_id)\n",
    "    result = evaluate(model, this_dataset, print_res=False)\n",
    "    average_pred = np.mean(this_dataset[:, -1])\n",
    "    all_stats.append([*result, 1.0 - average_pred])\n",
    "av_res = np.mean(all_stats, axis=0)\n",
    "print('Average Stats Before Training : ', av_res)\n",
    "train_setwise = {k: setwise_dataset[k] for k in train_keys}\n",
    "model, stats = set_wise_trainer(model, optimizer, train_setwise, num_epochs=N_EPOCHS)\n",
    "\n",
    "print('Logging Post-Training Performance')\n",
    "all_stats = []\n",
    "for id_, set_id in enumerate(val_keys):\n",
    "    if (id_ + 1) % int(len(val_keys) // 5) == 0:\n",
    "        print(id_, np.array(all_stats).mean(axis=0))\n",
    "    this_dataset = setwise_dataset[set_id]\n",
    "    model.remap_embedders(this_dataset, set_id)\n",
    "    result = evaluate(model, this_dataset, print_res=False)\n",
    "    average_pred = np.mean(this_dataset[:, -1])\n",
    "    all_stats.append([*result, 1.0 - average_pred])\n",
    "av_res = np.mean(all_stats, axis=0)\n",
    "print('Average Stats After Training : ', av_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
