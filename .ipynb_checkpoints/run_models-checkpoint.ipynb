{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code implements the training pipeline for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from dataformatter import *\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "\t# Esp important for ensuring deterministic behavior with CNNs\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\tcuda_available = torch.cuda.is_available()\n",
    "\tif cuda_available:\n",
    "\t\ttorch.cuda.manual_seed_all(seed)\n",
    "\treturn cuda_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, data_iterator, mode='train', eval_frac=-1):\n",
    "    model.train()\n",
    "    if mode == 'val':\n",
    "        model.eval()\n",
    "    stats = []\n",
    "    max_tr_batch = -1\n",
    "    num_egs, batch_idx, n_ones = 0, 1, 0\n",
    "    for batch in data_iterator:\n",
    "        # we get the loss from passing the batch to the model\n",
    "        # each model will have it's own way of deadling with the data [we can jointly figure this out]\n",
    "        if eval_frac > 0:\n",
    "            batch, num_batches = batch\n",
    "            max_tr_batch = int(eval_frac * num_batches)\n",
    "            n_ones = (np.array(batch)[:, -1]).sum()\n",
    "        loss, acc, bsz = model(batch)\n",
    "        stats.append([loss.item(), acc.item(), bsz, n_ones, len(batch)])\n",
    "        if mode == 'train' and ((batch_idx < max_tr_batch) or (max_tr_batch < 0)):\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "        batch_idx += 1\n",
    "    stats = np.array(stats)\n",
    "    avg_loss = np.mean(stats[:, 0])\n",
    "    avg_acc = (stats[:, 1]).sum() / (stats[:, 2].sum() * 1.0)\n",
    "    return (avg_loss, avg_acc), stats[max_tr_batch:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, dataset, num_epochs=20, desc='Description of model', eval_frac=0.8, shuffle=True):\n",
    "    # Todo [all]\n",
    "    # Figure out how to split the data into a train-val-test regime\n",
    "    stats = []\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle, batch_info=True)\n",
    "        epoch_stats, e_stats = run_epoch(model, optimizer, data_iter, mode='train', eval_frac=eval_frac)\n",
    "        stats.append(epoch_stats)\n",
    "        print('Epoch {} : Avrg Loss = {}, Avrg Acc = {} '.format(epoch_, stats[-1][0], stats[-1][1]))\n",
    "        major_acc = (e_stats[:, 3].sum()) / (1.0 * e_stats[:, 4].sum())\n",
    "        major_acc = max(major_acc, 1.0 - major_acc)\n",
    "        print('Epoch {} : Eval Loss = {}, Eval Acc = {}, Eval Majority Acc = {}'.format(epoch_, (e_stats[:, 0]).mean(), (e_stats[:, 1].sum())/(1.0 * e_stats[:, 2].sum()), major_acc))\n",
    "        print('-'*50)\n",
    "\n",
    "    stats = np.array(stats)\n",
    "#     graph_results(stats, desc)\n",
    "    return model\n",
    "\n",
    "def set_wise_trainer(model, optimizer, setwise_dataset, num_epochs=20, desc='Set-Wise Model', shuffle=False):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for epoch_ in range(num_epochs):\n",
    "        # get a data iterator for this epoch\n",
    "        accs = []\n",
    "        setwise_keys = list(setwise_dataset.keys())\n",
    "        perm = np.random.permutation(len(setwise_keys))\n",
    "        setwise_keys = np.array(setwise_keys)[perm]\n",
    "        for set_id  in setwise_keys:\n",
    "            dataset = setwise_dataset[set_id]\n",
    "            model.remap_embedders(dataset, set_id)\n",
    "            data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle)\n",
    "            this_stats, _ = run_epoch(model, optimizer, data_iter, mode='train')\n",
    "            stats[set_id].append(this_stats)\n",
    "            accs.append(this_stats[-1])\n",
    "        acc_stats = np.min(accs), np.mean(accs), np.median(accs), np.max(accs)\n",
    "        print('Min Acc {} | Mean Acc : {} | Median Acc {} | Max Acc {} '.format(*acc_stats))\n",
    "    return model, stats\n",
    "\n",
    "def evaluate(model, dataset, epoch_=-1, print_res=True, shuffle=False):\n",
    "    # get a data iterator for this epoch\n",
    "    model.eval()\n",
    "    data_iter = get_batch_iterator(dataset, BATCH_SZ, shuffle=shuffle, batch_info=False)\n",
    "    epoch_stats, _ = run_epoch(model, None, data_iter, mode='test')\n",
    "    if print_res:\n",
    "        print('Epoch {} : Avg Loss = {} Avg Acc = {}'.format(epoch_, epoch_stats[0], epoch_stats[1]))\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_setwise(eval_setwise_dataset, model, MAX_TR_KEY=0, desc='graph', epoch=-1):\n",
    "    all_stats = []\n",
    "    for id_, (set_id, this_dataset) in enumerate(eval_setwise_dataset.items()):\n",
    "        set_id = MAX_TR_KEY + int(set_id)\n",
    "        model.remap_embedders(this_dataset, set_id)\n",
    "        result = evaluate(model, this_dataset, print_res=False)\n",
    "        average_pred = np.mean(this_dataset[:, -1])\n",
    "        all_stats.append([*result, 1.0 - average_pred, average_pred])\n",
    "    av_res = np.mean(all_stats, axis=0)\n",
    "    print('[{}] Epoch[{}] : Loss {}, Acc {}, Major [0] Acc {}, Marjor [1] Acc {}'.format(desc, epoch, *av_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'hawkeye_trace_belady_graph.csv' # This is the CSV FILE WE ARE TRYING TO ANALYZE\n",
    "TR_DESC = 'GRAPH'\n",
    "EVAL_DATA_PATH = 'hawkeye_trace_belady_xalancbmk.csv' # This is the CSV FILE WE ARE TRYING TO ANALYZE\n",
    "EVAL_DESC = 'XALANCBMK'\n",
    "SAVE_FLDR = 'results'\n",
    "N_EPOCHS = 5\n",
    "MAX_GRAD_NORM = 0.1\n",
    "SET_WISE = True\n",
    "RANDOM_SEED = 140982301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_main():\n",
    "    if not os.path.exists(SAVE_FLDR):\n",
    "        os.makedirs(SAVE_FLDR)\n",
    "\n",
    "    set_random_seed(RANDOM_SEED)\n",
    "    print('Creating Model of type : {}, Batchsz = {}, Learning Rate = {}'.format(MODEL_TYPE, BATCH_SZ, LR))\n",
    "    model = get_model(MODEL_TYPE)\n",
    "    chosen_columns = model.get_data_columns()\n",
    "    train_dataset = csv_to_data(DATA_PATH, chosen_columns)\n",
    "    average_pred = np.mean(train_dataset[:, -1])\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.use_cuda = torch.cuda.is_available()\n",
    "    eval_dataset = csv_to_data(EVAL_DATA_PATH, chosen_columns)\n",
    "    print(model.feat_idx_map, torch.cuda.is_available())\n",
    "    print('This is the average accuracy : ', 1.0 - average_pred, ' From predicting all zeros')\n",
    "    \n",
    "    if not SET_WISE:\n",
    "        optimizer = Adam(model.parameters(), lr=LR)\n",
    "        model.prep_for_data(train_dataset, temp_order=True)\n",
    "        model = trainer(model, optimizer, train_dataset, num_epochs=N_EPOCHS, desc=MODEL_DESC, shuffle=False)\n",
    "    else:\n",
    "        train_setwise_dataset = group_by_set(train_dataset)\n",
    "        eval_setwise_dataset = group_by_set(eval_dataset)\n",
    "        all_tr_keys = list(train_setwise_dataset.keys())\n",
    "        val_keys = np.random.choice(all_tr_keys, size=int(0.2 * len(all_tr_keys)))\n",
    "        tr_keys = set(all_tr_keys) - set(val_keys)\n",
    "\n",
    "        vals = [len(x) for x in list(train_setwise_dataset.values())]\n",
    "        max_key = all_tr_keys[np.argmax(vals)]\n",
    "        # Logging-pre-training performance\n",
    "        model.prep_for_data(train_setwise_dataset[max_key], temp_order=True)\n",
    "        for set_id, this_dataset in train_setwise_dataset.items():\n",
    "            model.remap_embedders(this_dataset, set_id)\n",
    "\n",
    "        tr_val_setwise_dataset = {k: train_setwise_dataset[k] for k in val_keys}\n",
    "        train_setwise_dataset = {k: train_setwise_dataset[k] for k in tr_keys}\n",
    "\n",
    "        print('Logging Pre-Training Performance')\n",
    "        MAX_TR_KEY = max([int(x) for x in train_setwise_dataset.keys()]) + 1\n",
    "        eval_setwise(eval_setwise_dataset, model, MAX_TR_KEY=MAX_TR_KEY, desc=EVAL_DESC)\n",
    "        eval_setwise(tr_val_setwise_dataset, model, desc=TR_DESC)\n",
    "        optimizer = Adam(model.parameters(), lr=LR) # Now we can add all the model parameters to the optimizer\n",
    "        for i in range(N_EPOCHS):\n",
    "            model, stats = set_wise_trainer(model, optimizer, train_setwise_dataset, num_epochs=1)\n",
    "            eval_setwise(eval_setwise_dataset, model, MAX_TR_KEY=MAX_TR_KEY, desc=EVAL_DESC, epoch=i)\n",
    "            eval_setwise(tr_val_setwise_dataset, model, desc=TR_DESC, epoch=i)\n",
    "            torch.save(model.state_dict(), '{}/{}_saved_model.pth'.format(SAVE_FLDR, MODEL_DESC))\n",
    "    # Neeed to return the eval performance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model of type : TRANSFORMER, Batchsz = 32, Learning Rate = 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c26af8a5a825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mMODEL_TYPE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mMODEL_DESC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}-{}_BSZ.{}_LR.{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTR_DESC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmodel_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d23d3b63b23a>\u001b[0m in \u001b[0;36mmodel_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVAL_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_idx_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is the average accuracy : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maverage_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' From predicting all zeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldery/computer_architecture/15740_Project/dataformatter.py\u001b[0m in \u001b[0;36mcsv_to_data\u001b[0;34m(fname, chosen_columns)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcsv_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Program Counter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Set Occupancy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cache Friendly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mheader_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdf_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/ldery/anaconda3/envs/meta4da/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_szs = [32] #, 64]\n",
    "lrs = [1e-4] #, 3e-4]\n",
    "model_types = ['TRANSFORMER'] #, \"TRANSFORMER_1\", \"TRANSFORMER_2\"]\n",
    "SAVE_FLDR = \"pytorch_c++\"\n",
    "\n",
    "for BATCH_SZ in batch_szs:\n",
    "    for LR in lrs:\n",
    "        for MODEL_TYPE in model_types:\n",
    "            MODEL_DESC = \"{}-{}_BSZ.{}_LR.{}\".format(TR_DESC, MODEL_TYPE, BATCH_SZ, LR)\n",
    "            model_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_columns = ['Program Counter', 'Physical Address', 'Set', 'Cache Friendly']\n",
    "dataset = csv_to_data(DATA_PATH, chosen_columns)\n",
    "pcs = dataset[:, 0]\n",
    "dict_ = defaultdict(int)\n",
    "for id_ in pcs:\n",
    "    dict_[id_] += 1\n",
    "values = np.array(list(dict_.values()))\n",
    "bc = np.bincount(values)\n",
    "print(len(values), dataset.shape, values.mean(), values.max(), values.min(), np.median(values))\n",
    "print(bc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
